{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.mobilenet_v2 import MobileNetV2 \n",
    "from keras.applications.mobilenet import MobileNet\n",
    "from keras.applications.densenet import DenseNet169\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.nasnet import NASNetMobile, NASNetLarge\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.optimizers import Adam, SGD, Adagrad, Adadelta\n",
    "from keras import backend as K\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import random\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "import math\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET_PATH = r\"G:\\landmarks_of_melbourne_sml\"\n",
    "DATASET_PATH = r\"E:\\dataset15\"\n",
    "TESTSET_PATH = r\"E:\\testset15\"\n",
    "\n",
    "# initialize the number of epochs to train for, initial learning rate, batch size, and image dimensions\n",
    "EPOCHS = 15\n",
    "INIT_LR = 0.001\n",
    "BS = 8\n",
    "LABEL_NUM = 15\n",
    "READING_BS = 1000\n",
    "\n",
    "# image dimension may need to change according to the model spec\n",
    "IMAGE_DIMS =  (224, 224, 3)\n",
    "IMAGE_DIMS =  (299, 299, 3)\n",
    "\n",
    "# initialize the data and labels\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "data_test =[]\n",
    "labels_test=[]\n",
    "\n",
    "# grab the image paths and randomly shuffle them\n",
    "imagePaths = sorted(list(paths.list_images(DATASET_PATH)))\n",
    "random.Random(881278).shuffle(imagePaths)\n",
    "\n",
    "testPaths = sorted(list(paths.list_images(TESTSET_PATH)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data matrix: 1565.22MB\n"
     ]
    }
   ],
   "source": [
    "# loop over the test images\n",
    "for testPath in testPaths:\n",
    "    # load the image, pre-process it, and store it in the data_test list\n",
    "    assert os.path.isfile(testPath)\n",
    "    image = cv2.imread(testPath)\n",
    "    image = cv2.resize(image, (IMAGE_DIMS[1], IMAGE_DIMS[0]))\n",
    "    image = img_to_array(image)\n",
    "    data_test.append(image)\n",
    "\n",
    "    # extract the class label from the image path and update the labels_test list\n",
    "    label = testPath.split(os.path.sep)[-2]\n",
    "    labels_test.append(label)\n",
    "    \n",
    "# scale the raw pixel intensities to the range [0, 1]\n",
    "data_test = np.array(data_test, dtype=\"float\") / 255.0\n",
    "labels_test = np.array(labels_test)\n",
    "print(\"test data matrix: {:.2f}MB\".format(data_test.nbytes / (1024 * 1000.0)))\n",
    "\n",
    "# binarize the labels\n",
    "lb = LabelBinarizer()\n",
    "labels_test = lb.fit_transform(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\PC-user\\AppData\\Local\\conda\\conda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87916544/87910968 [==============================] - 45s 1us/step\n"
     ]
    }
   ],
   "source": [
    "# create a custom input layer that accepts images of shape IMAGE_DIMS\n",
    "input_tensor = Input(shape=IMAGE_DIMS)\n",
    "\n",
    "# grab the Keras built-in model for transfer learning\n",
    "base_model = InceptionV3(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_tensor=input_tensor,\n",
    "        input_shape=IMAGE_DIMS,\n",
    "        pooling='avg')\n",
    "\n",
    "# original model is trained on different dataset with different number of classes\n",
    "# so we need to append our own output layer\n",
    "output_tensor = Dense(LABEL_NUM, activation='softmax')(base_model.output)\n",
    "\n",
    "model = Model(inputs=input_tensor, outputs=output_tensor)\n",
    "\n",
    "model.compile(Adam(lr=1e-4), loss=\"categorical_crossentropy\", metrics=['accuracy', 'top_k_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data matrix: 2095.34MB\n",
      "WARNING:tensorflow:From C:\\Users\\PC-user\\AppData\\Local\\conda\\conda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/15\n",
      "125/125 [==============================] - 40s 318ms/step - loss: 0.9030 - acc: 0.7610 - top_k_categorical_accuracy: 0.9270 - val_loss: 0.2168 - val_acc: 0.9531 - val_top_k_categorical_accuracy: 0.9906\n",
      "Epoch 2/15\n",
      "125/125 [==============================] - 23s 184ms/step - loss: 0.1935 - acc: 0.9490 - top_k_categorical_accuracy: 0.9980 - val_loss: 0.1337 - val_acc: 0.9652 - val_top_k_categorical_accuracy: 0.9933\n",
      "Epoch 3/15\n",
      "125/125 [==============================] - 23s 186ms/step - loss: 0.0992 - acc: 0.9730 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1131 - val_acc: 0.9732 - val_top_k_categorical_accuracy: 0.9933\n",
      "Epoch 4/15\n",
      "125/125 [==============================] - 23s 186ms/step - loss: 0.0707 - acc: 0.9810 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1319 - val_acc: 0.9719 - val_top_k_categorical_accuracy: 0.9920\n",
      "Epoch 5/15\n",
      "125/125 [==============================] - 23s 184ms/step - loss: 0.0613 - acc: 0.9820 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1453 - val_acc: 0.9692 - val_top_k_categorical_accuracy: 0.9893\n",
      "Epoch 6/15\n",
      "125/125 [==============================] - 23s 184ms/step - loss: 0.0401 - acc: 0.9900 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1309 - val_acc: 0.9732 - val_top_k_categorical_accuracy: 0.9933\n",
      "Epoch 7/15\n",
      "125/125 [==============================] - 23s 184ms/step - loss: 0.0489 - acc: 0.9880 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.2602 - val_acc: 0.9384 - val_top_k_categorical_accuracy: 0.9880\n",
      "Epoch 8/15\n",
      "125/125 [==============================] - 23s 185ms/step - loss: 0.0682 - acc: 0.9810 - top_k_categorical_accuracy: 0.9990 - val_loss: 0.2137 - val_acc: 0.9398 - val_top_k_categorical_accuracy: 0.9893\n",
      "Epoch 9/15\n",
      "125/125 [==============================] - 23s 184ms/step - loss: 0.0398 - acc: 0.9860 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.2195 - val_acc: 0.9518 - val_top_k_categorical_accuracy: 0.9880\n",
      "Epoch 10/15\n",
      "125/125 [==============================] - 23s 185ms/step - loss: 0.0359 - acc: 0.9920 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1990 - val_acc: 0.9598 - val_top_k_categorical_accuracy: 0.9920\n",
      "Epoch 11/15\n",
      "125/125 [==============================] - 23s 184ms/step - loss: 0.0563 - acc: 0.9830 - top_k_categorical_accuracy: 0.9990 - val_loss: 0.2007 - val_acc: 0.9558 - val_top_k_categorical_accuracy: 0.9853\n",
      "Epoch 12/15\n",
      "125/125 [==============================] - 23s 184ms/step - loss: 0.0456 - acc: 0.9860 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1788 - val_acc: 0.9665 - val_top_k_categorical_accuracy: 0.9920\n",
      "Epoch 13/15\n",
      "125/125 [==============================] - 24s 191ms/step - loss: 0.0354 - acc: 0.9880 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1630 - val_acc: 0.9652 - val_top_k_categorical_accuracy: 0.9893\n",
      "Epoch 14/15\n",
      "125/125 [==============================] - 24s 195ms/step - loss: 0.0601 - acc: 0.9820 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1837 - val_acc: 0.9505 - val_top_k_categorical_accuracy: 0.9933\n",
      "Epoch 15/15\n",
      "125/125 [==============================] - 23s 184ms/step - loss: 0.0407 - acc: 0.9900 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1377 - val_acc: 0.9665 - val_top_k_categorical_accuracy: 0.9906\n",
      "training data matrix: 2095.34MB\n",
      "Epoch 1/15\n",
      "125/125 [==============================] - 23s 187ms/step - loss: 0.2697 - acc: 0.9300 - top_k_categorical_accuracy: 0.9880 - val_loss: 0.4293 - val_acc: 0.9009 - val_top_k_categorical_accuracy: 0.9732\n",
      "Epoch 2/15\n",
      "125/125 [==============================] - 24s 192ms/step - loss: 0.1556 - acc: 0.9600 - top_k_categorical_accuracy: 0.9970 - val_loss: 0.0952 - val_acc: 0.9732 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 3/15\n",
      "125/125 [==============================] - 24s 188ms/step - loss: 0.0688 - acc: 0.9790 - top_k_categorical_accuracy: 0.9980 - val_loss: 0.0924 - val_acc: 0.9705 - val_top_k_categorical_accuracy: 0.9946\n",
      "Epoch 4/15\n",
      "125/125 [==============================] - 24s 188ms/step - loss: 0.0670 - acc: 0.9800 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1519 - val_acc: 0.9639 - val_top_k_categorical_accuracy: 0.9987\n",
      "Epoch 5/15\n",
      "125/125 [==============================] - 23s 183ms/step - loss: 0.0466 - acc: 0.9890 - top_k_categorical_accuracy: 0.9990 - val_loss: 0.1151 - val_acc: 0.9692 - val_top_k_categorical_accuracy: 0.9973\n",
      "Epoch 6/15\n",
      "125/125 [==============================] - 23s 183ms/step - loss: 0.0252 - acc: 0.9940 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1189 - val_acc: 0.9732 - val_top_k_categorical_accuracy: 0.9920\n",
      "Epoch 7/15\n",
      "125/125 [==============================] - 23s 184ms/step - loss: 0.0391 - acc: 0.9860 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1285 - val_acc: 0.9652 - val_top_k_categorical_accuracy: 0.9960\n",
      "Epoch 8/15\n",
      "125/125 [==============================] - 23s 186ms/step - loss: 0.0502 - acc: 0.9830 - top_k_categorical_accuracy: 0.9990 - val_loss: 0.2187 - val_acc: 0.9545 - val_top_k_categorical_accuracy: 0.9960\n",
      "Epoch 9/15\n",
      "125/125 [==============================] - 23s 185ms/step - loss: 0.0265 - acc: 0.9920 - top_k_categorical_accuracy: 0.9990 - val_loss: 0.0835 - val_acc: 0.9746 - val_top_k_categorical_accuracy: 0.9987\n",
      "Epoch 10/15\n",
      "125/125 [==============================] - 23s 184ms/step - loss: 0.0174 - acc: 0.9970 - top_k_categorical_accuracy: 0.9990 - val_loss: 0.0691 - val_acc: 0.9786 - val_top_k_categorical_accuracy: 0.9973\n",
      "Epoch 11/15\n",
      "125/125 [==============================] - 23s 185ms/step - loss: 0.0359 - acc: 0.9930 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0873 - val_acc: 0.9719 - val_top_k_categorical_accuracy: 0.9946\n",
      "Epoch 12/15\n",
      "125/125 [==============================] - 23s 184ms/step - loss: 0.0124 - acc: 0.9980 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0891 - val_acc: 0.9799 - val_top_k_categorical_accuracy: 0.9960\n",
      "Epoch 13/15\n",
      "125/125 [==============================] - 23s 187ms/step - loss: 0.0153 - acc: 0.9950 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1535 - val_acc: 0.9652 - val_top_k_categorical_accuracy: 0.9973\n",
      "Epoch 14/15\n",
      "125/125 [==============================] - 23s 186ms/step - loss: 0.0287 - acc: 0.9900 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0839 - val_acc: 0.9786 - val_top_k_categorical_accuracy: 0.9960\n",
      "Epoch 15/15\n",
      "125/125 [==============================] - 23s 188ms/step - loss: 0.0186 - acc: 0.9930 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1283 - val_acc: 0.9719 - val_top_k_categorical_accuracy: 0.9946\n",
      "training data matrix: 2095.34MB\n",
      "Epoch 1/15\n",
      "125/125 [==============================] - 23s 186ms/step - loss: 0.2276 - acc: 0.9290 - top_k_categorical_accuracy: 0.9940 - val_loss: 0.2543 - val_acc: 0.9438 - val_top_k_categorical_accuracy: 0.9933\n",
      "Epoch 2/15\n",
      "125/125 [==============================] - 23s 183ms/step - loss: 0.1109 - acc: 0.9710 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1704 - val_acc: 0.9679 - val_top_k_categorical_accuracy: 0.9933\n",
      "Epoch 3/15\n",
      "125/125 [==============================] - 23s 185ms/step - loss: 0.0487 - acc: 0.9850 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1668 - val_acc: 0.9639 - val_top_k_categorical_accuracy: 0.9960\n",
      "Epoch 4/15\n",
      "125/125 [==============================] - 23s 183ms/step - loss: 0.0274 - acc: 0.9920 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1782 - val_acc: 0.9665 - val_top_k_categorical_accuracy: 0.9906\n",
      "Epoch 5/15\n",
      "125/125 [==============================] - 23s 185ms/step - loss: 0.0192 - acc: 0.9950 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1043 - val_acc: 0.9786 - val_top_k_categorical_accuracy: 0.9987\n",
      "Epoch 6/15\n",
      "125/125 [==============================] - 23s 184ms/step - loss: 0.0138 - acc: 0.9980 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1249 - val_acc: 0.9732 - val_top_k_categorical_accuracy: 0.9987\n",
      "Epoch 7/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 23s 184ms/step - loss: 0.0093 - acc: 0.9980 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1028 - val_acc: 0.9813 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 8/15\n",
      "125/125 [==============================] - 23s 184ms/step - loss: 0.0147 - acc: 0.9960 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0890 - val_acc: 0.9813 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 9/15\n",
      "125/125 [==============================] - 23s 184ms/step - loss: 0.0517 - acc: 0.9850 - top_k_categorical_accuracy: 0.9990 - val_loss: 0.2139 - val_acc: 0.9652 - val_top_k_categorical_accuracy: 0.9893\n",
      "Epoch 10/15\n",
      "125/125 [==============================] - 23s 183ms/step - loss: 0.0366 - acc: 0.9890 - top_k_categorical_accuracy: 0.9990 - val_loss: 0.0927 - val_acc: 0.9826 - val_top_k_categorical_accuracy: 0.9973\n",
      "Epoch 11/15\n",
      "125/125 [==============================] - 23s 182ms/step - loss: 0.0219 - acc: 0.9930 - top_k_categorical_accuracy: 0.9990 - val_loss: 0.1192 - val_acc: 0.9679 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 12/15\n",
      "125/125 [==============================] - 23s 183ms/step - loss: 0.0084 - acc: 0.9980 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0753 - val_acc: 0.9853 - val_top_k_categorical_accuracy: 0.9987\n",
      "Epoch 13/15\n",
      "125/125 [==============================] - 23s 182ms/step - loss: 0.0229 - acc: 0.9930 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1041 - val_acc: 0.9799 - val_top_k_categorical_accuracy: 0.9987\n",
      "Epoch 14/15\n",
      "125/125 [==============================] - 23s 182ms/step - loss: 0.0118 - acc: 0.9960 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0978 - val_acc: 0.9813 - val_top_k_categorical_accuracy: 0.9960\n",
      "Epoch 15/15\n",
      "125/125 [==============================] - 23s 183ms/step - loss: 0.0235 - acc: 0.9970 - top_k_categorical_accuracy: 0.9990 - val_loss: 0.1189 - val_acc: 0.9786 - val_top_k_categorical_accuracy: 1.0000\n",
      "training data matrix: 2095.34MB\n",
      "Epoch 1/15\n",
      "125/125 [==============================] - 23s 185ms/step - loss: 0.2021 - acc: 0.9510 - top_k_categorical_accuracy: 0.9890 - val_loss: 0.3325 - val_acc: 0.9103 - val_top_k_categorical_accuracy: 0.9920\n",
      "Epoch 2/15\n",
      "125/125 [==============================] - 23s 182ms/step - loss: 0.0932 - acc: 0.9700 - top_k_categorical_accuracy: 0.9990 - val_loss: 0.1036 - val_acc: 0.9799 - val_top_k_categorical_accuracy: 0.9960\n",
      "Epoch 3/15\n",
      "125/125 [==============================] - 23s 184ms/step - loss: 0.0843 - acc: 0.9700 - top_k_categorical_accuracy: 0.9990 - val_loss: 0.0698 - val_acc: 0.9772 - val_top_k_categorical_accuracy: 0.9973\n",
      "Epoch 4/15\n",
      "125/125 [==============================] - 23s 184ms/step - loss: 0.0499 - acc: 0.9850 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0569 - val_acc: 0.9880 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 5/15\n",
      "125/125 [==============================] - 23s 183ms/step - loss: 0.0307 - acc: 0.9930 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0703 - val_acc: 0.9799 - val_top_k_categorical_accuracy: 0.9973\n",
      "Epoch 6/15\n",
      "125/125 [==============================] - 23s 184ms/step - loss: 0.0235 - acc: 0.9950 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0463 - val_acc: 0.9880 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 7/15\n",
      "125/125 [==============================] - 23s 183ms/step - loss: 0.0180 - acc: 0.9930 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0623 - val_acc: 0.9853 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 8/15\n",
      "125/125 [==============================] - 23s 183ms/step - loss: 0.0138 - acc: 0.9950 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0516 - val_acc: 0.9880 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 9/15\n",
      "125/125 [==============================] - 23s 184ms/step - loss: 0.0146 - acc: 0.9950 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0621 - val_acc: 0.9906 - val_top_k_categorical_accuracy: 0.9987\n",
      "Epoch 10/15\n",
      "125/125 [==============================] - 23s 187ms/step - loss: 0.0071 - acc: 0.9990 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0577 - val_acc: 0.9866 - val_top_k_categorical_accuracy: 0.9987\n",
      "Epoch 11/15\n",
      "125/125 [==============================] - 23s 186ms/step - loss: 0.0067 - acc: 1.0000 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0438 - val_acc: 0.9880 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 12/15\n",
      "125/125 [==============================] - 24s 189ms/step - loss: 0.0072 - acc: 0.9990 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0521 - val_acc: 0.9866 - val_top_k_categorical_accuracy: 0.9987\n",
      "Epoch 13/15\n",
      "125/125 [==============================] - 23s 187ms/step - loss: 0.0045 - acc: 0.9990 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0371 - val_acc: 0.9893 - val_top_k_categorical_accuracy: 0.9987\n",
      "Epoch 14/15\n",
      "125/125 [==============================] - 23s 186ms/step - loss: 0.0276 - acc: 0.9920 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.4807 - val_acc: 0.9170 - val_top_k_categorical_accuracy: 0.9746\n",
      "Epoch 15/15\n",
      "125/125 [==============================] - 23s 186ms/step - loss: 0.0475 - acc: 0.9890 - top_k_categorical_accuracy: 0.9980 - val_loss: 0.1016 - val_acc: 0.9786 - val_top_k_categorical_accuracy: 0.9973\n",
      "training data matrix: 2095.34MB\n",
      "Epoch 1/15\n",
      "125/125 [==============================] - 24s 192ms/step - loss: 0.2019 - acc: 0.9370 - top_k_categorical_accuracy: 0.9950 - val_loss: 0.2456 - val_acc: 0.9357 - val_top_k_categorical_accuracy: 0.9946\n",
      "Epoch 2/15\n",
      "125/125 [==============================] - 23s 187ms/step - loss: 0.1077 - acc: 0.9700 - top_k_categorical_accuracy: 0.9990 - val_loss: 0.0832 - val_acc: 0.9759 - val_top_k_categorical_accuracy: 0.9960\n",
      "Epoch 3/15\n",
      "125/125 [==============================] - 23s 186ms/step - loss: 0.0567 - acc: 0.9820 - top_k_categorical_accuracy: 0.9990 - val_loss: 0.0592 - val_acc: 0.9933 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 4/15\n",
      "125/125 [==============================] - 23s 187ms/step - loss: 0.0622 - acc: 0.9840 - top_k_categorical_accuracy: 0.9980 - val_loss: 0.0669 - val_acc: 0.9880 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 5/15\n",
      "125/125 [==============================] - 23s 186ms/step - loss: 0.0181 - acc: 0.9970 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0502 - val_acc: 0.9893 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 6/15\n",
      "125/125 [==============================] - 23s 187ms/step - loss: 0.0129 - acc: 0.9950 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0488 - val_acc: 0.9880 - val_top_k_categorical_accuracy: 0.9987\n",
      "Epoch 7/15\n",
      "125/125 [==============================] - 23s 186ms/step - loss: 0.0104 - acc: 0.9970 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0398 - val_acc: 0.9933 - val_top_k_categorical_accuracy: 0.9987\n",
      "Epoch 8/15\n",
      "125/125 [==============================] - 23s 188ms/step - loss: 0.0058 - acc: 0.9990 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0712 - val_acc: 0.9866 - val_top_k_categorical_accuracy: 0.9987\n",
      "Epoch 9/15\n",
      "125/125 [==============================] - 23s 185ms/step - loss: 0.0051 - acc: 0.9990 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0543 - val_acc: 0.9893 - val_top_k_categorical_accuracy: 0.9973\n",
      "Epoch 10/15\n",
      "125/125 [==============================] - 23s 184ms/step - loss: 0.0036 - acc: 1.0000 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0496 - val_acc: 0.9906 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 11/15\n",
      "125/125 [==============================] - 23s 184ms/step - loss: 0.0069 - acc: 0.9970 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0379 - val_acc: 0.9920 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 12/15\n",
      "125/125 [==============================] - 23s 183ms/step - loss: 0.0289 - acc: 0.9910 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0686 - val_acc: 0.9853 - val_top_k_categorical_accuracy: 0.9987\n",
      "Epoch 13/15\n",
      "125/125 [==============================] - 23s 185ms/step - loss: 0.0134 - acc: 0.9950 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0728 - val_acc: 0.9826 - val_top_k_categorical_accuracy: 0.9987\n",
      "Epoch 14/15\n",
      "125/125 [==============================] - 23s 185ms/step - loss: 0.0253 - acc: 0.9910 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0623 - val_acc: 0.9866 - val_top_k_categorical_accuracy: 0.9973\n",
      "Epoch 15/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 23s 185ms/step - loss: 0.0303 - acc: 0.9880 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1146 - val_acc: 0.9692 - val_top_k_categorical_accuracy: 1.0000\n",
      "training data matrix: 1569.41MB\n",
      "Epoch 1/15\n",
      "94/94 [==============================] - 22s 239ms/step - loss: 0.3381 - acc: 0.9141 - top_k_categorical_accuracy: 0.9827 - val_loss: 0.4720 - val_acc: 0.8849 - val_top_k_categorical_accuracy: 0.9866\n",
      "Epoch 2/15\n",
      "94/94 [==============================] - 19s 205ms/step - loss: 0.1630 - acc: 0.9561 - top_k_categorical_accuracy: 0.9947 - val_loss: 0.1341 - val_acc: 0.9612 - val_top_k_categorical_accuracy: 0.9973\n",
      "Epoch 3/15\n",
      "94/94 [==============================] - 19s 206ms/step - loss: 0.0817 - acc: 0.9814 - top_k_categorical_accuracy: 0.9987 - val_loss: 0.0593 - val_acc: 0.9839 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 4/15\n",
      "94/94 [==============================] - 19s 204ms/step - loss: 0.0290 - acc: 0.9960 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0463 - val_acc: 0.9880 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 5/15\n",
      "94/94 [==============================] - 19s 207ms/step - loss: 0.0192 - acc: 0.9960 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0431 - val_acc: 0.9866 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 6/15\n",
      "94/94 [==============================] - 19s 205ms/step - loss: 0.0142 - acc: 0.9960 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0421 - val_acc: 0.9853 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 7/15\n",
      "94/94 [==============================] - 19s 202ms/step - loss: 0.0131 - acc: 0.9947 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0296 - val_acc: 0.9893 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 8/15\n",
      "94/94 [==============================] - 19s 202ms/step - loss: 0.0066 - acc: 0.9987 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0398 - val_acc: 0.9853 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 9/15\n",
      "94/94 [==============================] - 19s 201ms/step - loss: 0.0027 - acc: 1.0000 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0331 - val_acc: 0.9866 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 10/15\n",
      "94/94 [==============================] - 19s 201ms/step - loss: 0.0022 - acc: 1.0000 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0324 - val_acc: 0.9880 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 11/15\n",
      "94/94 [==============================] - 19s 204ms/step - loss: 0.0019 - acc: 1.0000 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0297 - val_acc: 0.9893 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 12/15\n",
      "94/94 [==============================] - 19s 201ms/step - loss: 0.0060 - acc: 0.9973 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0511 - val_acc: 0.9799 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 13/15\n",
      "94/94 [==============================] - 19s 200ms/step - loss: 0.0020 - acc: 1.0000 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0436 - val_acc: 0.9839 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 14/15\n",
      "94/94 [==============================] - 19s 200ms/step - loss: 0.0141 - acc: 0.9947 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0534 - val_acc: 0.9813 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 15/15\n",
      "94/94 [==============================] - 19s 200ms/step - loss: 0.0057 - acc: 1.0000 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0393 - val_acc: 0.9826 - val_top_k_categorical_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "chunk_paths = [imagePaths[i:i + READING_BS] for i in range(0, len(imagePaths), READING_BS)]\n",
    "for chunk_path in chunk_paths:\n",
    "    # loop over the each chuck\n",
    "    data = []\n",
    "    labels = []\n",
    "    for imagePath in chunk_path:\n",
    "        # load the image, pre-process it, and store it in the data list\n",
    "        assert os.path.isfile(imagePath)\n",
    "        image = cv2.imread(imagePath)\n",
    "        image = cv2.resize(image, (IMAGE_DIMS[1], IMAGE_DIMS[0]))\n",
    "        image = img_to_array(image)\n",
    "        data.append(image)\n",
    "\n",
    "        # extract the class label from the image path and update the labels list\n",
    "        label = imagePath.split(os.path.sep)[-2]\n",
    "        labels.append(label)\n",
    "    \n",
    "    # construct the image generator for data augmentation\n",
    "    # aug = ImageDataGenerator(rotation_range=25, width_shift_range=0.1, height_shift_range=0.1, \n",
    "    #                          shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "    aug = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1)\n",
    "    \n",
    "    # scale the raw pixel intensities to the range [0, 1]\n",
    "    data = np.array(data, dtype=\"float\") / 255.0\n",
    "    labels = np.array(labels)\n",
    "    print(\"training data matrix: {:.2f}MB\".format(data.nbytes / (1024 * 1000.0)))\n",
    "\n",
    "    # binarize the labels\n",
    "    lb = LabelBinarizer()\n",
    "    labels = lb.fit_transform(labels)\n",
    "    \n",
    "    h = model.fit_generator(\n",
    "        aug.flow(data, labels, batch_size=BS),\n",
    "        validation_data=(data_test, labels_test),\n",
    "        steps_per_epoch=math.ceil(len(data)/BS),\n",
    "        epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "model.save(\"E:\\models\\InceptionV3-dataset15-d299-e15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the label binarizer to disk\n",
    "f = open(r\"E:\\models\\lb-dataset15-i\", \"wb\")\n",
    "f.write(pickle.dumps(lb))\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
